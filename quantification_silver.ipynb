{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import pickle\n",
    "from gensim import models\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.stats as stats\n",
    "\n",
    "# load embeddings\n",
    "w = models.KeyedVectors.load_word2vec_format(\n",
    "    'embedding_files/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "ft = fasttext.load_model('embedding_files/crawl-300d-2M-subword.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# skip next cell for faster runtime and to load models here if you do not wish to retrain the models\n",
    "linear_model_w = pickle.load(open('silver_standard/model_savefiles/linear_model_w.sav', 'rb'))\n",
    "linear_model_ft = pickle.load(open('silver_standard/model_savefiles/linear_model_ft.sav', 'rb'))\n",
    "mlp_model_w = pickle.load(open('silver_standard/model_savefiles/mlp_model_w.sav', 'rb'))\n",
    "mlp_model_ft = pickle.load(open('silver_standard/model_savefiles/mlp_model_ft.sav', 'rb'))\n",
    "knn_model_ft = pickle.load(open('silver_standard/model_savefiles/knn_model_ft.sav', 'rb'))\n",
    "knn_model_w = pickle.load(open('silver_standard/model_savefiles/knn_model_w.sav', 'rb'))\n",
    "svr_model_w = pickle.load(open('silver_standard/model_savefiles/svr_model_w.sav', 'rb'))\n",
    "svr_model_ft = pickle.load(open('silver_standard/model_savefiles/svr_model_ft.sav', 'rb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# skip this cell if loaded models previously\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tabulate import tabulate\n",
    "\n",
    "# train with train and test data\n",
    "x_w = [] #  for embeddings w2vec\n",
    "y= [] # for scores\n",
    "x_ft = [] # for fasttext\n",
    "words_text = [] # for words\n",
    "\n",
    "with open('silver_standard/predictions/predictions_overall.txt', 'r') as f:  # load in silver standard file\n",
    "    for line in f:\n",
    "        word, v = line.split('\\t')\n",
    "        # w2vec embedding\n",
    "        try:\n",
    "            emb = w[word]\n",
    "        except KeyError:\n",
    "            emb = np.zeros(300) # if embedding not found, use zero embedding\n",
    "        x_w.append(emb)\n",
    "\n",
    "        # fasttext embedding\n",
    "        embedding = ft.get_word_vector(word)\n",
    "        x_ft.append(embedding)\n",
    "\n",
    "        y.append(float(v)) # use valency as target\n",
    "        words_text.append(word)\n",
    "\n",
    "    X_w = np.array(x_w)\n",
    "    X_ft = np.array(x_ft)\n",
    "\n",
    "    # random state number ensures that split is identical\n",
    "    X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y, test_size=0.1, random_state=20, shuffle=True)\n",
    "    X_train_ft, X_test_ft, y_train_ft, y_test_ft = train_test_split(X_ft, y, test_size=0.1, random_state=20, shuffle=True)\n",
    "\n",
    "\n",
    "    # w2vec models\n",
    "    # Linear Regression\n",
    "    linear_model_w = LinearRegression()\n",
    "    linear_model_w.fit(X_train_w, y_train_w)\n",
    "    y_pred_linear_w = linear_model_w.predict(np.array(X_test_w))\n",
    "\n",
    "    # MLP Regressor\n",
    "    mlp_model_w =  MLPRegressor(random_state=1, max_iter=200)\n",
    "    mlp_model_w.fit(X_train_w, y_train_w) # fit model w2vec\n",
    "    y_pred_mlp_w = mlp_model_w.predict(np.array(X_test_w))\n",
    "\n",
    "    # KNN Regressor\n",
    "    knn_model_w = KNeighborsRegressor(n_neighbors=10)\n",
    "    knn_model_w.fit(X_train_w, y_train_w)\n",
    "\n",
    "    # SVR Regressor\n",
    "    svr_model_w = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    svr_model_w.fit(X_train_w, y_train_w)\n",
    "\n",
    "\n",
    "    # fasttext models\n",
    "    # Linear Regression\n",
    "    linear_model_ft = LinearRegression()\n",
    "    linear_model_ft.fit(X_train_ft, y_train_ft)\n",
    "    y_pred_linear_ft = linear_model_ft.predict(X_test_ft)\n",
    "\n",
    "    # MLP Regressor\n",
    "    mlp_model_ft =  MLPRegressor(random_state=1, max_iter=200)\n",
    "    mlp_model_ft.fit(X_train_ft, y_train_ft)\n",
    "    y_pred_mlp_ft = mlp_model_ft.predict(X_test_ft)\n",
    "\n",
    "    # KNN Regressor\n",
    "    knn_model_ft = KNeighborsRegressor(n_neighbors=5)\n",
    "    knn_model_ft.fit(X_train_ft, y_train_ft)\n",
    "\n",
    "    # SVM Regressor\n",
    "    # svr_model_ft = SVR(kernel='rbf')\n",
    "    svr_model_ft = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    svr_model_ft.fit(X_train_ft, y_train_ft)\n",
    "\n",
    "\n",
    "print('FASTTEXT')\n",
    "print('on test set')\n",
    "mean_squared_error = mean_squared_error\n",
    "table = [['', 'linear', 'mlp', 'knn', 'svr'], ['mse', mean_squared_error(y_test_ft, linear_model_ft.predict(X_test_ft)), mean_squared_error(y_test_ft, mlp_model_ft.predict(X_test_ft)), mean_squared_error(y_test_ft, knn_model_ft.predict(X_test_ft)), mean_squared_error(y_test_ft, svr_model_ft.predict(X_test_ft))], ['r2 score', r2_score(y_test_ft, linear_model_ft.predict(X_test_ft)), r2_score(y_test_ft, mlp_model_ft.predict(X_test_ft)), r2_score(y_test_ft, knn_model_ft.predict(X_test_ft)), r2_score(y_test_ft, svr_model_ft.predict(X_test_ft))]]\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
    "print('on training set')\n",
    "table = [['', 'linear', 'mlp', 'knn', 'svr'], ['mse', mean_squared_error(y_train_ft, linear_model_ft.predict(X_train_ft)), mean_squared_error(y_train_ft, mlp_model_ft.predict(X_train_ft)), mean_squared_error(y_train_ft, knn_model_ft.predict(X_train_ft)), mean_squared_error(y_train_ft, svr_model_ft.predict(X_train_ft))], ['r2 score', r2_score(y_train_ft, linear_model_ft.predict(X_train_ft)), r2_score(y_train_ft, mlp_model_ft.predict(X_train_ft)), r2_score(y_train_ft, knn_model_ft.predict(X_train_ft)), r2_score(y_train_ft, svr_model_ft.predict(X_train_ft))]]\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
    "\n",
    "print('W2VEC')\n",
    "print('on test set')\n",
    "table = [['', 'linear', 'mlp', 'knn', 'svr'], ['mse', mean_squared_error(y_test_w, linear_model_w.predict(X_test_w)), mean_squared_error(y_test_w, mlp_model_w.predict(X_test_w)), mean_squared_error(y_test_w, knn_model_w.predict(X_test_w)), mean_squared_error(y_test_w, svr_model_w.predict(X_test_w))], ['r2 score', r2_score(y_test_w, linear_model_w.predict(X_test_w)), r2_score(y_test_w, mlp_model_w.predict(X_test_w)), r2_score(y_test_w, knn_model_w.predict(X_test_w)), r2_score(y_test_w, svr_model_w.predict(X_test_w))]]\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
    "print('on training set')\n",
    "table = [['', 'linear', 'mlp', 'knn', 'svr'], ['mse', mean_squared_error(y_train_w, linear_model_w.predict(X_train_w)), mean_squared_error(y_train_w, mlp_model_w.predict(X_train_w)), mean_squared_error(y_train_w, knn_model_w.predict(X_train_w)), mean_squared_error(y_train_w, svr_model_w.predict(X_train_w))], ['r2 score', r2_score(y_train_w, linear_model_w.predict(X_train_w)), r2_score(y_train_w, mlp_model_w.predict(X_train_w)), r2_score(y_train_w, knn_model_w.predict(X_train_w)), r2_score(y_train_w, svr_model_w.predict(X_train_w))]]\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# average embeddings, then produce score\n",
    "# here, it would be possible to tweak the score the model gives, e.g. by inverting score if there's a negation\n",
    "def process_phrase(phr, model, model_type='w'):\n",
    "    model_score = 0\n",
    "    embedding_sum = np.zeros(300)\n",
    "    wrds = phr.split(' ')\n",
    "    if model_type == 'w':\n",
    "        for wrd in wrds:\n",
    "            try:\n",
    "                em = w[wrd]\n",
    "            except KeyError: # zero vector for unknown word\n",
    "                em = np.zeros(300, dtype=np.float32)\n",
    "            embedding_sum += em\n",
    "        embedding_avg = embedding_sum / len(wrds) # average embedding\n",
    "        embedding_avg = embedding_avg.reshape(1, -1)\n",
    "        embedding_sum = embedding_sum.reshape(1, -1)\n",
    "        model_score =  model.predict(embedding_avg) # either write embedding_avg or embedding_sum depending on what you want to calculate\n",
    "    if model_type == 'ft':\n",
    "        for wrd in wrds:\n",
    "            em = ft.get_word_vector(wrd)\n",
    "            em = np.array(em)\n",
    "            embedding_sum += em\n",
    "        embedding_avg = embedding_sum / len(wrds) # average embedding\n",
    "        embedding_avg = embedding_avg.reshape(1, -1)\n",
    "        embedding_sum = embedding_sum.reshape(1, -1)\n",
    "        model_score =  model.predict(embedding_avg) # either write embedding_avg or embedding_sum depending on what you want to calculate\n",
    "    return model_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pred_kendall(scores_phr_pred, original_scores_phr, outfile):\n",
    "    # writes predictions into file and calculates kendall\n",
    "    scores_idx_split = []\n",
    "    scores_phr_pred.sort(key=lambda s: s[1], reverse=True)\n",
    "    for triplet in scores_phr_pred:\n",
    "        scores_idx_split.append(triplet[2])\n",
    "        outfile.write(triplet[0] + '\\t' + str(triplet[1]) + '\\n')\n",
    "\n",
    "    y_idx = []\n",
    "    for triple in original_scores_phr:\n",
    "        y_idx.append(triple[2])\n",
    "\n",
    "    # kendall tau\n",
    "    tau, pvalue = stats.kendalltau(scores_idx_split, y_idx)\n",
    "    # calculate spearman\n",
    "    spearman, pvalue_s = stats.spearmanr(scores_idx_split, y_idx)\n",
    "    return f'\\nkendall: {tau}\\t pvalue: {pvalue}\\nspearman: {spearman}\\tpvalue: {pvalue_s}'\n",
    "\n",
    "def pred_write_only(scores_phr_pred, file):\n",
    "    # writes predictions into file\n",
    "    scores_phr_pred.sort(key=lambda s: s[1], reverse=True)\n",
    "    for triplet in scores_phr_pred:\n",
    "        file.write(triplet[0].rstrip('\\n') + '\\t' + str(triplet[1]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2vec \n",
      "\n",
      "linear \n",
      "kendall: 0.5484872699097835\t pvalue: 0.0\n",
      "spearman: 0.7524495245581392\tpvalue: 0.0 \n",
      "\n",
      "mlp \n",
      "kendall: 0.524574920942101\t pvalue: 0.0\n",
      "spearman: 0.7232058536682534\tpvalue: 0.0 \n",
      "\n",
      "knn \n",
      "kendall: 0.3953533389464889\t pvalue: 8.318771747688187e-216\n",
      "spearman: 0.5397130083620549\tpvalue: 2.1192639751423615e-211 \n",
      "\n",
      "svr \n",
      "kendall: 0.549138988421526\t pvalue: 0.0\n",
      "spearman: 0.746536148761682\tpvalue: 0.0 \n",
      "\n",
      "\n",
      "fasttext \n",
      "\n",
      "linear \n",
      "kendall: 0.4214986920939037\t pvalue: 5.277350794893067e-245\n",
      "spearman: 0.6040903897166524\tpvalue: 4.4440540043726076e-278 \n",
      "\n",
      "mlp \n",
      "kendall: 0.4455678416753048\t pvalue: 1.5633814014785723e-273\n",
      "spearman: 0.6278847420270555\tpvalue: 8.483362394529516e-307 \n",
      "\n",
      "knn \n",
      "kendall: 0.4806778996174729\t pvalue: 0.0\n",
      "spearman: 0.6671978858403246\tpvalue: 0.0 \n",
      "\n",
      "svr \n",
      "kendall: 0.4333210497673401\t pvalue: 8.079058769365877e-259\n",
      "spearman: 0.6108247737819159\tpvalue: 5.854223730161609e-286 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_scores(model, filepath, model_type: str = 'w'):\n",
    "    phrases_and_scores = [] # original scores from training examples\n",
    "    scores = [] # scores from the model averaging score\n",
    "    idx = 0\n",
    "    with open(filepath) as file:\n",
    "        for line in file:\n",
    "            phrase, score = line.rstrip().split('\\t')\n",
    "            phrases_and_scores.append((phrase, score, idx))\n",
    "            score = process_phrase(phrase, model, model_type)\n",
    "            scores.append((phrase, score, idx))\n",
    "\n",
    "            idx += 1\n",
    "        return scores, phrases_and_scores\n",
    "\n",
    "\n",
    "with open('silver_standard/rankings/scl_nma/lin_w.txt', 'w+') as lin_w, \\\n",
    "        open('silver_standard/rankings/scl_nma/mlp_w.txt', 'w+') as mlp_w, \\\n",
    "        open('silver_standard/rankings/scl_nma/knn_w.txt', 'w+') as knn_w, \\\n",
    "        open('silver_standard/rankings/scl_nma/svr_w.txt', 'w+') as svr_w, \\\n",
    "        open('silver_standard/rankings/scl_nma/lin_ft.txt', 'w+') as lin_ft, \\\n",
    "        open('silver_standard/rankings/scl_nma/mlp_ft.txt', 'w+') as mlp_ft, \\\n",
    "        open('silver_standard/rankings/scl_nma/knn_ft.txt', 'w+') as knn_ft, \\\n",
    "        open('silver_standard/rankings/scl_nma/svr_ft.txt', 'w+') as svr_ft:\n",
    "    # testfilepath = 'silver_standard/scl_nma/SCL-NMA-single.txt' # single words only\n",
    "    #testfilepath = 'silver_standard/scl_nma/SCL-NMA-multiple.txt' # multi word phrases\n",
    "    # testfilepath = 'silver_standard/scl_nma/SCL-NMA.txt' # overall\n",
    "    testfilepath = 'silver_standard/scl_nma/SemEval2016-overall.txt' # overall\n",
    "    # testfilepath = 'silver_standard/scl_nma/SCL-NMA-single.txt' # single words only\n",
    "    # testfilepath = 'silver_standard/scl_nma/SCL-NMA-multiple.txt' # multi word phrases\n",
    "\n",
    "    print('w2vec', '\\n')\n",
    "    scores, phrases_and_scores = calculate_scores(linear_model_w, testfilepath, 'w')\n",
    "    print('linear', pred_kendall(scores, phrases_and_scores, lin_w), '\\n')\n",
    "\n",
    "    scores, phrases_and_scores = calculate_scores(mlp_model_w, testfilepath, 'w')\n",
    "    print('mlp', pred_kendall(scores, phrases_and_scores,mlp_w), '\\n')\n",
    "\n",
    "    scores, phrases_and_scores = calculate_scores(knn_model_w, testfilepath, 'w')\n",
    "    print('knn', pred_kendall(scores, phrases_and_scores,knn_w), '\\n')\n",
    "\n",
    "    scores, phrases_and_scores = calculate_scores(svr_model_w, testfilepath, 'w')\n",
    "    print('svr', pred_kendall(scores, phrases_and_scores,svr_w), '\\n\\n')\n",
    "\n",
    "\n",
    "    print('fasttext', '\\n')\n",
    "    scores, phrases_and_scores = calculate_scores(linear_model_ft, testfilepath, 'ft')\n",
    "    print('linear', pred_kendall(scores, phrases_and_scores, lin_ft), '\\n')\n",
    "\n",
    "    scores, phrases_and_scores = calculate_scores(mlp_model_ft, testfilepath, 'ft')\n",
    "    print('mlp', pred_kendall(scores, phrases_and_scores,mlp_ft), '\\n')\n",
    "\n",
    "    scores, phrases_and_scores = calculate_scores(knn_model_ft, testfilepath, 'ft')\n",
    "    print('knn', pred_kendall(scores, phrases_and_scores,knn_ft), '\\n')\n",
    "\n",
    "    scores, phrases_and_scores = calculate_scores(svr_model_ft, testfilepath, 'ft')\n",
    "    print('svr', pred_kendall(scores, phrases_and_scores,svr_ft), '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_score(infilepath, model, model_type: str = 'w'):\n",
    "    # calculate scores for file without initial scores\n",
    "    scores = []\n",
    "    idx = 0\n",
    "    with open(infilepath, 'r') as file:\n",
    "        for line in file:\n",
    "            score = process_phrase(line.rstrip(), model, model_type)\n",
    "            scores.append((line, score))\n",
    "            idx += 1\n",
    "        return scores\n",
    "\n",
    "\n",
    "# sample without given scores\n",
    "with open('silver_standard/rankings/bbc_news/lin_w.txt', 'w+') as lin_w, \\\n",
    "        open('silver_standard/rankings/bbc_news/mlp_w.txt', 'w+') as mlp_w, \\\n",
    "        open('silver_standard/rankings/bbc_news/knn_w.txt', 'w+') as knn_w, \\\n",
    "        open('silver_standard/rankings/bbc_news/svr_w.txt', 'w+') as svr_w, \\\n",
    "        open('silver_standard/rankings/bbc_news/lin_ft.txt', 'w+') as lin_ft, \\\n",
    "        open('silver_standard/rankings/bbc_news/mlp_ft.txt', 'w+') as mlp_ft, \\\n",
    "        open('silver_standard/rankings/bbc_news/knn_ft.txt', 'w+') as knn_ft, \\\n",
    "        open('silver_standard/rankings/bbc_news/svr_ft.txt', 'w+') as svr_ft:\n",
    "    infilepath = 'silver_standard/np_extraction/nps_news_clean_300_examples.txt'\n",
    "    save_paths = [lin_w, mlp_w, knn_w, svr_w, lin_ft, mlp_ft, knn_ft, svr_ft]\n",
    "\n",
    "    model_saves = [linear_model_w, mlp_model_w, knn_model_w, svr_model_w,\n",
    "                   linear_model_ft, mlp_model_ft, knn_model_ft, svr_model_ft]\n",
    "\n",
    "    idx = 0\n",
    "    for model, savepath in zip(model_saves, save_paths):\n",
    "        if idx <= 3:\n",
    "            scores = calculate_score(infilepath, model, 'w')\n",
    "        else:\n",
    "            scores = calculate_score(infilepath, model, 'ft')\n",
    "        pred_write_only(scores, savepath)\n",
    "        idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pickle.dump(linear_model_w, open('silver_standard/model_savefiles/linear_model_w.sav', 'wb'))\n",
    "# pickle.dump(linear_model_ft, open('silver_standard/model_savefiles/linear_model_ft.sav', 'wb'))\n",
    "# pickle.dump(mlp_model_ft, open('silver_standard/model_savefiles/mlp_model_ft.sav', 'wb'))\n",
    "# pickle.dump(mlp_model_w, open('silver_standard/model_savefiles/mlp_model_w.sav', 'wb'))\n",
    "# pickle.dump(knn_model_w, open('silver_standard/model_savefiles/knn_model_w.sav', 'wb'))\n",
    "# pickle.dump(knn_model_ft, open('silver_standard/model_savefiles/knn_model_ft.sav', 'wb'))\n",
    "# pickle.dump(svr_model_w, open('silver_standard/model_savefiles/svr_model_w.sav', 'wb'))\n",
    "# pickle.dump(svr_model_ft, open('silver_standard/model_savefiles/svr_model_ft.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}